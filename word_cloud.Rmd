---
title: "Assessment"
author: "AKW"
date: "2025-03-31"
output: html_document
---
# Loading packages and importing dataset
```{r}

library(readxl)
library(tm)
library(tidytext)
library(wordcloud)
library(SnowballC)
library(dplyr)
library(tidyr)
library(ggplot2)
mydata <- read_excel("~/mydata.xlsx")


```

# cleaning of data
```{r}
# Convert text to corpus
corpus <- Corpus(VectorSource(mydata$Gen_Comment))

# Text cleaning: Convert to lowercase, remove punctuation, numbers, and stopwords
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stripWhitespace)


```

# Term document matrix
```{r}
# creating a TMD

tdm <- TermDocumentMatrix(corpus)

# Convert TDM to a matrix
tdm_matrix <- as.matrix(tdm)

# Sum word frequencies
word_freqs <- sort(rowSums(tdm_matrix), decreasing = TRUE)

# Convert to data frame
word_freq_df <- data.frame(word = names(word_freqs), freq = word_freqs)

# View top words
head(word_freq_df, 10)

```

# visualization of word cloud
```{r}
set.seed(123)  # For reproducibility
wordcloud(words = word_freq_df$word, 
          freq = word_freq_df$freq, 
          min.freq = 2, 
          max.words = 100, 
          colors = brewer.pal(8, "Dark2"), 
          random.order = FALSE)

```


```{r}
# Tokenize into bigrams
bigrams <- mydata %>%
  unnest_tokens(bigram, Gen_Comment, token = "ngrams", n = 2)

# Count bigram frequency
bigram_counts <- bigrams %>%
  count(bigram, sort = TRUE)

# View top bigrams
head(bigram_counts, 10)

# Separate bigrams into two words
bigram_separated <- bigram_counts %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Remove common stopwords
stop_words_list <- stop_words$word
bigram_cleaned <- bigram_separated %>%
  filter(!word1 %in% stop_words_list & !word2 %in% stop_words_list) %>%
  unite(bigram, word1, word2, sep = " ")  # Recombine words

# View cleaned bigrams
head(bigram_cleaned, 10)


```






